{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dietary-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fifth-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gen_input(self, batch_size=32, noise_size=62, n_class=10, seed=None):\n",
    "  # create noise input\n",
    "  noise = tf.random.normal([batch_size, noise_size], seed=seed)\n",
    "  # Create categorical latent code\n",
    "  label = tf.random.uniform([batch_size], minval=0, maxval=10, dtype=tf.int32, seed=seed)\n",
    "  label = tf.one_hot(label, depth=n_class)\n",
    "  # Create one continuous latent code\n",
    "  c_1 = tf.random.uniform([batch_size, 1], minval=-1, maxval=1, seed=seed)\n",
    "  return label, c_1, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "owned-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator_continuous(n_filters=128, input_size=73):\n",
    "    # Build functional API model\n",
    "    # input\n",
    "    input = keras.layers.Input(shape=(input_size, ))\n",
    "\n",
    "    # Fully-connected layer.\n",
    "    dense_1 = keras.layers.Dense(units=1024, use_bias=False) (input)\n",
    "    bn_1 = keras.layers.BatchNormalization()(dense_1)\n",
    "    act_1 = keras.layers.ReLU()(bn_1)\n",
    "    # Fully-connected layer. The output should be able to reshape into 7x7\n",
    "    dense_2 = keras.layers.Dense(units=7*7*128, use_bias=False) (act_1)\n",
    "    bn_2 = keras.layers.BatchNormalization()(dense_2)\n",
    "    act_2 = keras.layers.ReLU()(bn_2)\n",
    "    # Reshape\n",
    "    reshape = keras.layers.Reshape(target_shape=(7, 7, 128))(act_2)\n",
    "\n",
    "    nf = n_filters\n",
    "    # First transposed convolutional layer\n",
    "\n",
    "    tc_1 = keras.layers.Conv2DTranspose(nf, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False)(reshape)\n",
    "    bn_1 = keras.layers.BatchNormalization()(tc_1)\n",
    "    act_1 = keras.layers.ReLU()(bn_1)\n",
    "\n",
    "    # Number of filters halved after each transposed convolutional layer\n",
    "    nf = nf//2\n",
    "    # Second transposed convolutional layer\n",
    "    # strides=(2, 2): shape is doubled after the transposed convolution\n",
    "    tc_2 = keras.layers.Conv2DTranspose(nf, kernel_size=(4, 4), strides=(2, 2), padding='same', use_bias=False)(act_1)\n",
    "    bn_2 = keras.layers.BatchNormalization()(tc_2)\n",
    "    act_2 = keras.layers.ReLU()(bn_2)\n",
    "\n",
    "    # Final transposed convolutional layer: output shape: 28x28x1, tanh activation\n",
    "    output = keras.layers.Conv2DTranspose(1, kernel_size=(4, 4), strides=(1, 1), \n",
    "                                         padding=\"same\", activation=\"tanh\")(act_2)\n",
    "    model = keras.models.Model(inputs=input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "noted-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator_continuous(n_filters=64, n_class=10, input_shape=(28, 28, 1)):\n",
    "    # Build functional API model\n",
    "    # Image Input\n",
    "    image_input = keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    nf = n_filters\n",
    "    c_1 = keras.layers.Conv2D(nf, kernel_size=(4, 4), strides=(2, 2), padding=\"same\", use_bias=True)(image_input)\n",
    "    bn_1 = keras.layers.BatchNormalization()(c_1)\n",
    "    act_1 = keras.layers.LeakyReLU(alpha=0.1)(bn_1)\n",
    "\n",
    "    # Number of filters doubled after each convolutional layer\n",
    "    nf = nf*2\n",
    "    # Second convolutional layer\n",
    "    # Output shape: 7x7\n",
    "    c_2 = keras.layers.Conv2D(nf, kernel_size=(4, 4), strides=(2, 2), padding=\"same\", use_bias=False)(act_1)\n",
    "    bn_2 = keras.layers.BatchNormalization()(c_2)\n",
    "    act_2 = keras.layers.LeakyReLU(alpha=0.1)(bn_2)\n",
    "\n",
    "    # Flatten the convolutional layers\n",
    "    flatten = keras.layers.Flatten()(act_2)\n",
    "\n",
    "    # FC layer\n",
    "    dense = keras.layers.Dense(1024, use_bias=False)(flatten)\n",
    "    bn = keras.layers.BatchNormalization()(dense)\n",
    "    act = keras.layers.LeakyReLU(alpha=0.1)(bn)\n",
    "    # Discriminator output. Sigmoid activation function to classify \"True\" or \"False\"\n",
    "    d_output = keras.layers.Dense(1, activation='sigmoid')(act)\n",
    "\n",
    "    # Auxiliary output. \n",
    "    q_dense = keras.layers.Dense(128, use_bias=False)(act)\n",
    "    q_bn = keras.layers.BatchNormalization()(q_dense)\n",
    "    q_act = keras.layers.LeakyReLU(alpha=0.1)(q_bn)\n",
    "\n",
    "    # Classification (discrete output)\n",
    "    clf_out = keras.layers.Dense(n_class, activation=\"softmax\")(q_act)\n",
    "\n",
    "    # Gaussian distribution mean (continuous output)\n",
    "    mu = keras.layers.Dense(1)(q_act)\n",
    "\n",
    "    # Gaussian distribution standard deviation (exponential activation to ensure the value is positive)\n",
    "    sigma = keras.layers.Dense(1, activation=lambda x: tf.math.exp(x))(q_act)\n",
    "\n",
    "    # Discriminator model (not compiled)\n",
    "    d_model = keras.models.Model(inputs=image_input, outputs=d_output)\n",
    "\n",
    "    # Auxiliary model (not compiled)\n",
    "    q_model = keras.models.Model(inputs=image_input, outputs=[clf_out, mu, sigma])\n",
    "    return d_model, q_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "specialized-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoGAN_Continuous(keras.Model):\n",
    "    def __init__(self, d_model, g_model, q_model,noise_size, num_classes):\n",
    "        super(InfoGAN_Continuous, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.g_model = g_model\n",
    "        self.q_model = q_model\n",
    "        self.noise_size = noise_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, q_optimizer):\n",
    "        super(InfoGAN_Continuous, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.q_optimizer = q_optimizer\n",
    "\n",
    "    def create_gen_input(self, batch_size, noise_size, n_class, seed=None):\n",
    "        # create noise input\n",
    "        noise = tf.random.normal([batch_size, noise_size], seed=seed)\n",
    "        # Create categorical latent code\n",
    "        label = tf.random.uniform([batch_size], minval=0, maxval=10, dtype=tf.int32, seed=seed)\n",
    "        label = tf.one_hot(label, depth=n_class)\n",
    "        # Create one continuous latent code\n",
    "        c_1 = tf.random.uniform([batch_size, 1], minval=-1, maxval=1, seed=seed)\n",
    "        return label, c_1, noise\n",
    "\n",
    "    def concat_inputs(self, input):\n",
    "        concat_input = keras.layers.Concatenate()(input)\n",
    "        return concat_input\n",
    "\n",
    "    def train_step(self, real_image_batch):\n",
    "        # Define loss functions\n",
    "        binary_loss = keras.losses.BinaryCrossentropy()\n",
    "        categorical_loss = keras.losses.CategoricalCrossentropy()\n",
    "        # Half-batch for training discriminator and batch for training generator and auxiliary model\n",
    "        batch = tf.shape(real_image_batch)[0]\n",
    "        # Create generator input \n",
    "        g_label, c_1, g_noise = self.create_gen_input(batch, self.noise_size, self.num_classes, seed=None)\n",
    "        g_input = self.concat_inputs([g_label, c_1, g_noise])\n",
    "        with tf.GradientTape() as d_tape: \n",
    "            self.d_model.trainable = True\n",
    "            d_tape.watch(self.d_model.trainable_variables)\n",
    "            # Train discriminator using half batch real images\n",
    "            y_disc_real = tf.ones((batch, 1))\n",
    "            d_real_output = self.d_model(real_image_batch, training=True)\n",
    "            d_loss_real = binary_loss(y_disc_real, d_real_output)\n",
    "            # Train discriminator using half batch fake images     \n",
    "            y_disc_fake = tf.zeros((batch, 1))\n",
    "            # Create fake image batch\n",
    "            fake_image_batch = self.g_model(g_input, training=True)\n",
    "            d_fake_output = self.d_model(fake_image_batch, training=True)\n",
    "            d_loss_fake = binary_loss(y_disc_fake, d_fake_output)\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "        # Calculate gradients\n",
    "        d_gradients = d_tape.gradient(d_loss, self.d_model.trainable_variables)\n",
    "        # Optimize\n",
    "        self.d_optimizer.apply_gradients(zip(d_gradients, self.d_model.trainable_variables))\n",
    "        with tf.GradientTape() as g_tape, tf.GradientTape() as q_tape:\n",
    "            # Create generator input \n",
    "            g_label, c_1, g_noise = self.create_gen_input(batch*2, self.noise_size, self.num_classes, seed=None)\n",
    "            g_input = self.concat_inputs([g_label, c_1, g_noise])\n",
    "            g_tape.watch(self.g_model.trainable_variables)\n",
    "            q_tape.watch(self.q_model.trainable_variables)\n",
    "            # Create fake image batch\n",
    "            fake_image_batch = self.g_model(g_input, training=True)\n",
    "            d_fake_output = self.d_model(fake_image_batch, training=True)\n",
    "            # Generator Image loss\n",
    "            y_gen_fake = tf.ones((batch*2, 1))\n",
    "            g_img_loss = binary_loss(y_gen_fake, d_fake_output)\n",
    "            # Auxiliary loss\n",
    "            cat_output, mu, sigma = self.q_model(fake_image_batch, training=True)\n",
    "            # Categorical loss\n",
    "            cat_loss = categorical_loss(g_label, cat_output)\n",
    "            # Use Gaussian distributions to represent the output\n",
    "            dist = tfp.distributions.Normal(loc=mu, scale=sigma)\n",
    "            # Losses (negative log probability density function as we want to maximize the probability density function)\n",
    "            c_1_loss = tf.reduce_mean(-dist.log_prob(c_1))\n",
    "            # Generator total loss\n",
    "            g_loss = g_img_loss + (cat_loss + 0.1*c_1_loss)\n",
    "            # Auxiliary function loss\n",
    "            q_loss = (cat_loss + 0.1*c_1_loss)\n",
    "        # Calculate gradients\n",
    "        # We do not want to modify the neurons in the discriminator when training the generator and the auxiliary model\n",
    "        self.d_model.trainable=False\n",
    "        g_gradients = g_tape.gradient(g_loss, self.g_model.trainable_variables)\n",
    "        q_gradients = q_tape.gradient(q_loss, self.q_model.trainable_variables)\n",
    "        # Optimize\n",
    "        self.g_optimizer.apply_gradients(zip(g_gradients, self.g_model.trainable_variables))\n",
    "        self.q_optimizer.apply_gradients(zip(q_gradients, self.q_model.trainable_variables))\n",
    "\n",
    "        return {\"d_loss_real\": d_loss_real, \"d_loss_fake\": d_loss_fake, \"g_img_loss\": g_img_loss ,\n",
    "                \"cat_loss\": cat_loss, \"c_1_loss\": c_1_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "royal-pricing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1330s 707ms/step - d_loss_real: 0.4781 - d_loss_fake: 0.4628 - g_img_loss: 1.2399 - cat_loss: 0.2621 - c_1_loss: 0.8898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24042d2d730>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_real_image(batch_size=32):\n",
    "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "    # Add the color channel - change to 4D tensor, and convert the data type to 'float32'\n",
    "    train_images = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
    "    # Set the pixel values from -1 to 1\n",
    "    train_images = (train_images/255.0) * 2 - 1\n",
    "    # Shuffle and separate in batch\n",
    "    buffer_size = train_images.shape[0]\n",
    "    train_images_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(buffer_size).batch(batch_size)\n",
    "    return train_images_dataset\n",
    "\n",
    "g_model_continuous = create_generator_continuous()\n",
    "d_model_continuous, q_model_continuous = create_discriminator_continuous()\n",
    "infogan = InfoGAN_Continuous(d_model_continuous, g_model_continuous, q_model_continuous, noise_size=62, num_classes=10)\n",
    "infogan.compile(d_optimizer=keras.optimizers.Adam(learning_rate=2e-4),\n",
    "                g_optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n",
    "                q_optimizer=keras.optimizers.Adam(learning_rate=2e-4))\n",
    "real_images = load_real_image(batch_size=32)\n",
    "infogan.fit(real_images, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "improved-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(generator, n_samples):\n",
    "        # generate points in latent space and control codes\n",
    "        z_input, _ = generate_noise(n_samples)\n",
    "        # predict outputs\n",
    "        images = generator.predict(z_input)\n",
    "        # create class labels\n",
    "        y = zeros((n_samples, 1))\n",
    "        return images, y\n",
    "    \n",
    "def generate_noise(n_samples):\n",
    "        # generate points in the latent space\n",
    "        z_latent=randn(n_samples,10)\n",
    "        # generate categorical one-hot codes\n",
    "        cat_codes=np.eye(self.n_cat)[np.random.choice(self.n_cat, n_samples)]\n",
    "        # concatenate latent points and control codes\n",
    "        z_input = hstack((z_latent, cat_codes))\n",
    "        return [z_input, cat_codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "polar-organ",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate_fake_samples() missing 1 required positional argument: 'n_samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e3005f0e8c0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_fake_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfogan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: generate_fake_samples() missing 1 required positional argument: 'n_samples'"
     ]
    }
   ],
   "source": [
    "image, y = generate_fake_samples(infogan.g_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-problem",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
